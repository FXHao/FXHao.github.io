---
layout: post
title: "网络爬虫_1"
date: 2019-10-12
description: "网络爬虫"
tag: 爬虫
---

## 网络爬虫1

* **网络爬虫** 就是模拟客户端发送网络请求，接受请求响应，一种按照一定的规则，自动地抓取互联网信息的程序
* 请求有两种：一般**表单提交** 和 **超大文本** 的是**POST** 请求，其余的则为 **GET** 请求
* 响应状态码：
  * **100+**：服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程
  * **200+**：请求成功
  * **300+**：重定向
  * **400+**：客户端请求有错误
  * **500+**：服务器出错

### 发送请求

* 使用的是**request**模块，在Python里还有个模块**urllib**也可以完成相同的功能，但个人感觉request更为简便

  ```python
  import requests
  
  header = {
      "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36",
  }
  url = "https://www.baidu.com"
  
  response = requests.get(url, headers=header)
  ##############################################
  response.status_code  # 查看请求是否成功
  response.content # 获取页面内容（需要decode解码，网页是以bytes格式传输的）
  ```

### 使用代理

* 让服务器以为不是同一个客户在请求

* 使用代理ip前可以在request添加超时参数，来检测代理ip的质量好坏

  ```python
  import requests
  
  proxies = {"http": "http://139.129.207.72:808"}
  headers = {
      "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"}
  
  r = requests.get("http://www.baidu.com", proxies=proxies, headers=headers)
  ```

### 携带cookie请求

* request有一个**session**类，可实现客户端与服务器的会话保持

* 使用：实例化一个session对象，让其发送get或POST请求

  ```python
  session = request.session()
  response = session.get(url,headers)
  ```

### 获取登陆后页面的三种方式

* 实例化一个session，使用session发送POST请求，再使用它获取登陆后的页面

* headers中添加cookie键，值为cookie字符串

* 在请求方法中添加cookie参数，接受字典形式的cookie，字典形式中的键是cookie的name对应的值，值是cookie的value对应的值

  ```python
  headers = {
      "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"
  }
  
  cookies = "anonymid=jwfvkcx52x5zxv; depovince=ZGQT; _r01_=1; JSESSIONID=abcP3un1cbdaFbxNvMBSw; ick_login=ce45ed04-6675-4b4b-b67b-642fb42decb7; ick=12b6e8b9-415e-4a73-b8a2-29c2e92cd581"
  
  # 字典推导式把cookies转换成字典
  cookies = {i.split("=")[0]: i.split("=")[1] for i in cookies.split(";")}
  print(cookies)
  
  r = requests.get("http://www.renren.com/971048550", headers=headers, cookies=cookies)
  ```

### 寻找登陆的POST地址

* 在**from**表单中 **action** 对应的URL地址：POST数据为input标签中name所对应的值
* 可能有from表单没有action，这时就需要抓包：勾选perseve log 按钮，防止页面跳转，寻找POST数据，确定参数：
  * 参数不变，直接用
  * 参数变：
    * 参数在前面的响应中
    * **js**生成

### 定位想要的JS

* 选择触发JS事件的按钮，点event listener ，找到JS位置
* 通过chrome中的search all file 来搜索URL中的关键字
* 添加断点方式查看JS的操作，通过Python来进行同样的操作

### request小伎俩

* `response.cookies`  可以获取服务器写入本地的cookie，但得到的是一个对象，想要将其转换成一个字典给我们使用，可以

  ```python
  request.utils.dict_from_cookiejar(response.cookies) # 对象转换成字典
  request.utils.cookiejar_from_dict({...})  # 字典转换成对象
  ```

* **url编解码**

  ```python
  request.utils.unquote()  # 解码
  request.utils.quote()  # 编码
  ```

* 请求SSL证书验证

  请求时直接添加参数 `verify=False` ，不让其验证证书

  ```python
  request.get(url,verify=False)
  ```

* 超时参数

  ```python
  request.get(url,timeout=10)  # 超时时会报错，需要捕获异常
  ```

### json注意点

* json中字符串都是双引号引起来的
* 如果不是双引号：
  * `eval`：能实现简单的字符串和Python类型转换
  * `replace`：把单引号替换成双引号
* 在一个文件中写入多个json串，不在是一个json串，不能直读取

### lxml注意

* lxml可以修正HTML代码，但也有可能会修改错误，这时就可以通过`etree.tosring()`观察修改后的HTML，根据修改后的HTML的样子来写xpath

### xpath

```python
//a[text()='下一页']  # 选择文本为下一页的a标签
a/text()  # 获取a标签下的文本
a//text()  # 获取a标签下的所有文本
a/@href  # 获取a标签下的链接
ul[@class='tex']  # 选择class值为tex的标签
//div[contanins(@class,'i')]  # 选择class包含i的div
```

### 总结：爬虫流程

* 准备URL：
  * start_url：URL地址不明显，总数不确定，通过代码提取下一页URL
  * url_list：地址规律明显，页码总数明确
* 发送请求，获取响应
  * 随机的User-Agent，代理IP，反反爬虫
  * 在被对方识别为爬虫后，可准备cookie池
    * 如果不登陆：准备开始能够请求成功的cookie，即接受对方网站设置在response的cookie
    * 如果登陆：准备多个账号，实用程序获取每个账号的cookie
* 提取数据
  * 确定数据的位置：
    * 在当前URL地址中：直接提取，不用进详情页；提取详情页
    * 不在当前URL中：在其他响应中，找！
  * 提取：xpath(从HTML中提取整块的数据，先分组，之后每一组在提取)，json，re
* 保存数据
  * 保存本地：text，json，csv
  * 保存在数据库

